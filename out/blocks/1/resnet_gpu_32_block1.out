Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: matplotlib in /home/mnk2978/.local/lib/python3.9/site-packages (3.5.1)
Requirement already satisfied: fonttools>=4.22.0 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (4.31.2)
Requirement already satisfied: packaging>=20.0 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (21.3)
Requirement already satisfied: pyparsing>=2.2.1 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (3.0.7)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (1.4.0)
Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib) (1.22.2)
Requirement already satisfied: pillow>=6.2.0 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib) (9.0.1)
Requirement already satisfied: python-dateutil>=2.7 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: cycler>=0.10 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: six>=1.5 in /ext3/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Current Numpy Version- 1.22.2
Current Matplotlib Version- 3.5.1
Current Python Version- 3.9.7
Current Torch Version- 1.10.2+cu113
Current Torchvision Version- 0.11.3+cu113
Namespace(dataset='cifar10', model='resnet18', batch_size=128, epochs=200, learning_rate=0.1, data_augmentation=True, cutout=True, n_holes=1, length=16, no_cuda=False, seed=0, cuda=True)
Using basic data augmentation
Using data augmentation CUTOUT
Files already downloaded and verified
Files already downloaded and verified
Loading the model
Getting the model on GPU
Number of Parameters:  2114500
Making csv file for logs
[INFO] visualizing training batch...
Starting Training
test_acc: 0.533
test_acc: 0.518
test_acc: 0.638
test_acc: 0.641
test_acc: 0.693
test_acc: 0.720
test_acc: 0.774
test_acc: 0.764
test_acc: 0.676
test_acc: 0.740
test_acc: 0.747
test_acc: 0.788
test_acc: 0.790
test_acc: 0.735
test_acc: 0.781
test_acc: 0.781
test_acc: 0.764
test_acc: 0.766
test_acc: 0.802
test_acc: 0.820
test_acc: 0.727
test_acc: 0.811
test_acc: 0.816
test_acc: 0.744
test_acc: 0.768
test_acc: 0.830
test_acc: 0.800
test_acc: 0.785
test_acc: 0.800
test_acc: 0.835
test_acc: 0.775
test_acc: 0.778
test_acc: 0.804
test_acc: 0.805
test_acc: 0.763
test_acc: 0.759
test_acc: 0.764
test_acc: 0.797
test_acc: 0.814
test_acc: 0.805
test_acc: 0.769
test_acc: 0.803
test_acc: 0.813
test_acc: 0.713
test_acc: 0.817
test_acc: 0.814
test_acc: 0.826
test_acc: 0.845
test_acc: 0.799
test_acc: 0.803
test_acc: 0.829
test_acc: 0.680
test_acc: 0.776
test_acc: 0.750
test_acc: 0.836
test_acc: 0.799
test_acc: 0.807
test_acc: 0.780
test_acc: 0.818
test_acc: 0.825
test_acc: 0.897
test_acc: 0.897
test_acc: 0.904
test_acc: 0.902
test_acc: 0.905
test_acc: 0.902
test_acc: 0.908
test_acc: 0.906
test_acc: 0.890
test_acc: 0.906
test_acc: 0.905
test_acc: 0.907
test_acc: 0.897
test_acc: 0.889
test_acc: 0.895
test_acc: 0.891
test_acc: 0.898
test_acc: 0.900
test_acc: 0.894
test_acc: 0.899
test_acc: 0.858
test_acc: 0.900
test_acc: 0.900
test_acc: 0.885
test_acc: 0.885
test_acc: 0.897
test_acc: 0.898
test_acc: 0.895
test_acc: 0.904
test_acc: 0.904
test_acc: 0.879
test_acc: 0.886
test_acc: 0.893
test_acc: 0.884
test_acc: 0.894
test_acc: 0.860
test_acc: 0.892
test_acc: 0.899
test_acc: 0.904
test_acc: 0.903
test_acc: 0.905
test_acc: 0.896
test_acc: 0.898
test_acc: 0.900
test_acc: 0.898
test_acc: 0.904
test_acc: 0.900
test_acc: 0.897
test_acc: 0.895
test_acc: 0.896
test_acc: 0.895
test_acc: 0.896
test_acc: 0.883
test_acc: 0.906
test_acc: 0.901
test_acc: 0.905
test_acc: 0.914
test_acc: 0.902
test_acc: 0.895
test_acc: 0.892
test_acc: 0.927
test_acc: 0.930
test_acc: 0.932
test_acc: 0.929
test_acc: 0.933
test_acc: 0.930
test_acc: 0.933
test_acc: 0.932
test_acc: 0.935
test_acc: 0.934
test_acc: 0.931
test_acc: 0.934
test_acc: 0.933
test_acc: 0.934
test_acc: 0.934
test_acc: 0.935
test_acc: 0.934
test_acc: 0.935
test_acc: 0.932
test_acc: 0.931
test_acc: 0.930
test_acc: 0.930
test_acc: 0.926
test_acc: 0.934
test_acc: 0.931
test_acc: 0.933
test_acc: 0.932
test_acc: 0.932
test_acc: 0.935
test_acc: 0.932
test_acc: 0.930
test_acc: 0.930
test_acc: 0.931
test_acc: 0.930
test_acc: 0.936
test_acc: 0.929
test_acc: 0.930
test_acc: 0.933
test_acc: 0.930
test_acc: 0.933
test_acc: 0.940
test_acc: 0.939
test_acc: 0.939
test_acc: 0.940
test_acc: 0.940
test_acc: 0.941
test_acc: 0.940
test_acc: 0.940
test_acc: 0.941
test_acc: 0.941
test_acc: 0.940
test_acc: 0.940
test_acc: 0.942
test_acc: 0.940
test_acc: 0.941
test_acc: 0.941
test_acc: 0.939
test_acc: 0.941
test_acc: 0.939
test_acc: 0.941
test_acc: 0.941
test_acc: 0.940
test_acc: 0.941
test_acc: 0.941
test_acc: 0.942
test_acc: 0.940
test_acc: 0.941
test_acc: 0.941
test_acc: 0.940
test_acc: 0.940
test_acc: 0.939
test_acc: 0.941
test_acc: 0.941
test_acc: 0.941
test_acc: 0.941
test_acc: 0.942
test_acc: 0.940
test_acc: 0.941
test_acc: 0.940
test_acc: 0.943
