Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: matplotlib in /home/mnk2978/.local/lib/python3.9/site-packages (3.5.1)
Requirement already satisfied: cycler>=0.10 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (4.31.2)
Requirement already satisfied: python-dateutil>=2.7 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: packaging>=20.0 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (21.3)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (1.4.0)
Requirement already satisfied: pillow>=6.2.0 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib) (9.0.1)
Requirement already satisfied: pyparsing>=2.2.1 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (3.0.7)
Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib) (1.22.2)
Requirement already satisfied: six>=1.5 in /ext3/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Current Numpy Version- 1.22.2
Current Matplotlib Version- 3.5.1
Current Python Version- 3.9.7
Current Torch Version- 1.10.2+cu113
Current Torchvision Version- 0.11.3+cu113
Namespace(dataset='cifar10', model='resnet18', batch_size=128, epochs=200, learning_rate=0.1, data_augmentation=True, cutout=True, n_holes=1, length=16, no_cuda=False, seed=0, cuda=True)
Using basic data augmentation
Using data augmentation CUTOUT
Files already downloaded and verified
Files already downloaded and verified
Loading the model
Getting the model on GPU
Number of Parameters:  701466
Making csv file for logs
[INFO] visualizing training batch...
Starting Training
test_acc: 0.481
test_acc: 0.583
test_acc: 0.632
test_acc: 0.659
test_acc: 0.681
test_acc: 0.721
test_acc: 0.722
test_acc: 0.680
test_acc: 0.747
test_acc: 0.765
test_acc: 0.735
test_acc: 0.751
test_acc: 0.762
test_acc: 0.780
test_acc: 0.738
test_acc: 0.791
test_acc: 0.728
test_acc: 0.752
test_acc: 0.737
test_acc: 0.769
test_acc: 0.717
test_acc: 0.776
test_acc: 0.734
test_acc: 0.680
test_acc: 0.779
test_acc: 0.769
test_acc: 0.774
test_acc: 0.796
test_acc: 0.790
test_acc: 0.753
test_acc: 0.766
test_acc: 0.810
test_acc: 0.805
test_acc: 0.782
test_acc: 0.781
test_acc: 0.797
test_acc: 0.794
test_acc: 0.774
test_acc: 0.821
test_acc: 0.757
test_acc: 0.751
test_acc: 0.725
test_acc: 0.757
test_acc: 0.802
test_acc: 0.770
test_acc: 0.761
test_acc: 0.715
test_acc: 0.795
test_acc: 0.807
test_acc: 0.661
test_acc: 0.812
test_acc: 0.780
test_acc: 0.725
test_acc: 0.744
test_acc: 0.795
test_acc: 0.814
test_acc: 0.815
test_acc: 0.806
test_acc: 0.813
test_acc: 0.818
test_acc: 0.881
test_acc: 0.880
test_acc: 0.888
test_acc: 0.882
test_acc: 0.889
test_acc: 0.890
test_acc: 0.883
test_acc: 0.858
test_acc: 0.895
test_acc: 0.885
test_acc: 0.887
test_acc: 0.881
test_acc: 0.884
test_acc: 0.884
test_acc: 0.883
test_acc: 0.890
test_acc: 0.875
test_acc: 0.866
test_acc: 0.877
test_acc: 0.877
test_acc: 0.890
test_acc: 0.867
test_acc: 0.878
test_acc: 0.885
test_acc: 0.887
test_acc: 0.879
test_acc: 0.880
test_acc: 0.866
test_acc: 0.870
test_acc: 0.877
test_acc: 0.892
test_acc: 0.875
test_acc: 0.890
test_acc: 0.858
test_acc: 0.883
test_acc: 0.871
test_acc: 0.868
test_acc: 0.862
test_acc: 0.876
test_acc: 0.880
test_acc: 0.864
test_acc: 0.880
test_acc: 0.887
test_acc: 0.876
test_acc: 0.874
test_acc: 0.873
test_acc: 0.876
test_acc: 0.891
test_acc: 0.876
test_acc: 0.857
test_acc: 0.879
test_acc: 0.885
test_acc: 0.869
test_acc: 0.891
test_acc: 0.875
test_acc: 0.890
test_acc: 0.867
test_acc: 0.850
test_acc: 0.866
test_acc: 0.884
test_acc: 0.916
test_acc: 0.919
test_acc: 0.918
test_acc: 0.921
test_acc: 0.922
test_acc: 0.923
test_acc: 0.920
test_acc: 0.921
test_acc: 0.923
test_acc: 0.921
test_acc: 0.924
test_acc: 0.923
test_acc: 0.920
test_acc: 0.923
test_acc: 0.922
test_acc: 0.920
test_acc: 0.921
test_acc: 0.921
test_acc: 0.919
test_acc: 0.920
test_acc: 0.918
test_acc: 0.919
test_acc: 0.923
test_acc: 0.919
test_acc: 0.920
test_acc: 0.922
test_acc: 0.917
test_acc: 0.916
test_acc: 0.918
test_acc: 0.918
test_acc: 0.920
test_acc: 0.921
test_acc: 0.918
test_acc: 0.921
test_acc: 0.921
test_acc: 0.918
test_acc: 0.915
test_acc: 0.916
test_acc: 0.918
test_acc: 0.922
test_acc: 0.928
test_acc: 0.928
test_acc: 0.929
test_acc: 0.929
test_acc: 0.930
test_acc: 0.930
test_acc: 0.932
test_acc: 0.930
test_acc: 0.929
test_acc: 0.930
test_acc: 0.932
test_acc: 0.932
test_acc: 0.930
test_acc: 0.930
test_acc: 0.931
test_acc: 0.930
test_acc: 0.932
test_acc: 0.931
test_acc: 0.931
test_acc: 0.931
test_acc: 0.931
test_acc: 0.931
test_acc: 0.932
test_acc: 0.930
test_acc: 0.932
test_acc: 0.932
test_acc: 0.931
test_acc: 0.931
test_acc: 0.931
test_acc: 0.931
test_acc: 0.932
test_acc: 0.930
test_acc: 0.934
test_acc: 0.933
test_acc: 0.934
test_acc: 0.932
test_acc: 0.932
test_acc: 0.932
test_acc: 0.931
test_acc: 0.930
