Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: matplotlib in /home/mnk2978/.local/lib/python3.9/site-packages (3.5.1)
Requirement already satisfied: pyparsing>=2.2.1 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (3.0.7)
Requirement already satisfied: cycler>=0.10 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib) (1.22.2)
Requirement already satisfied: python-dateutil>=2.7 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: fonttools>=4.22.0 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (4.31.2)
Requirement already satisfied: packaging>=20.0 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (21.3)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/mnk2978/.local/lib/python3.9/site-packages (from matplotlib) (1.4.0)
Requirement already satisfied: pillow>=6.2.0 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib) (9.0.1)
Requirement already satisfied: six>=1.5 in /ext3/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Current Numpy Version- 1.22.2
Current Matplotlib Version- 3.5.1
Current Python Version- 3.9.7
Current Torch Version- 1.10.2+cu113
Current Torchvision Version- 0.11.3+cu113
Namespace(dataset='cifar10', model='resnet18', batch_size=64, epochs=200, learning_rate=0.1, data_augmentation=True, cutout=True, n_holes=1, length=16, no_cuda=False, seed=0, cuda=True)
Using basic data augmentation
Using data augmentation CUTOUT
Files already downloaded and verified
Files already downloaded and verified
Loading the model
Getting the model on GPU
Number of Parameters:  2797610
Making csv file for logs
[INFO] visualizing training batch...
Starting Training
test_acc: 0.422
test_acc: 0.536
test_acc: 0.533
test_acc: 0.446
test_acc: 0.595
test_acc: 0.620
test_acc: 0.678
test_acc: 0.686
test_acc: 0.692
test_acc: 0.695
test_acc: 0.723
test_acc: 0.725
test_acc: 0.587
test_acc: 0.729
test_acc: 0.695
test_acc: 0.754
test_acc: 0.722
test_acc: 0.696
test_acc: 0.709
test_acc: 0.626
test_acc: 0.642
test_acc: 0.745
test_acc: 0.746
test_acc: 0.778
test_acc: 0.780
test_acc: 0.756
test_acc: 0.754
test_acc: 0.746
test_acc: 0.692
test_acc: 0.673
test_acc: 0.777
test_acc: 0.757
test_acc: 0.779
test_acc: 0.774
test_acc: 0.754
test_acc: 0.783
test_acc: 0.747
test_acc: 0.748
test_acc: 0.744
test_acc: 0.717
test_acc: 0.762
test_acc: 0.722
test_acc: 0.646
test_acc: 0.756
test_acc: 0.736
test_acc: 0.721
test_acc: 0.740
test_acc: 0.768
test_acc: 0.775
test_acc: 0.714
test_acc: 0.640
test_acc: 0.783
test_acc: 0.761
test_acc: 0.760
test_acc: 0.800
test_acc: 0.743
test_acc: 0.803
test_acc: 0.791
test_acc: 0.757
test_acc: 0.804
test_acc: 0.877
test_acc: 0.875
test_acc: 0.880
test_acc: 0.872
test_acc: 0.862
test_acc: 0.880
test_acc: 0.875
test_acc: 0.879
test_acc: 0.877
test_acc: 0.887
test_acc: 0.867
test_acc: 0.884
test_acc: 0.882
test_acc: 0.860
test_acc: 0.892
test_acc: 0.853
test_acc: 0.883
test_acc: 0.859
test_acc: 0.868
test_acc: 0.875
test_acc: 0.883
test_acc: 0.873
test_acc: 0.888
test_acc: 0.876
test_acc: 0.882
test_acc: 0.888
test_acc: 0.884
test_acc: 0.879
test_acc: 0.878
test_acc: 0.887
test_acc: 0.879
test_acc: 0.873
test_acc: 0.885
test_acc: 0.883
test_acc: 0.892
test_acc: 0.878
test_acc: 0.884
test_acc: 0.877
test_acc: 0.873
test_acc: 0.890
test_acc: 0.879
test_acc: 0.886
test_acc: 0.880
test_acc: 0.886
test_acc: 0.888
test_acc: 0.877
test_acc: 0.879
test_acc: 0.888
test_acc: 0.889
test_acc: 0.893
test_acc: 0.883
test_acc: 0.872
